{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T07:53:19.190278Z",
     "start_time": "2020-04-29T07:53:14.833014Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 42865, Built 33785, Cleaned 28937, Transferred 28846\n",
      "Not downloaded 735, Not built 4294, Not cleaned 94, Not transferred 0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import urllib\n",
    "prefix = '/Users/dd/Downloads/.metagraph_run37/'\n",
    "\n",
    "fServer = open(f'{prefix}/server.log')\n",
    "lines = fServer.readlines()\n",
    "\n",
    "\n",
    "downloaded_sras = set()\n",
    "build_sras = set()\n",
    "clean_sras = set()\n",
    "transfer_sras = set()\n",
    "ndownloaded_sras = set()\n",
    "nbuild_sras = set()\n",
    "nclean_sras = set()\n",
    "ntransfer_sras = set()\n",
    "time_first = 0\n",
    "time_last = 0\n",
    "for l in lines:\n",
    "    if not 'ack/' in l:\n",
    "        continue\n",
    "    split_l = l.split(' ')\n",
    "    date_time = datetime.datetime.strptime(split_l[0] + ' ' + split_l[1], '%Y-%m-%d %H:%M:%S,%f')\n",
    "    parsed = urllib.parse.parse_qs(split_l[4])\n",
    "    \n",
    "    sra_id = parsed['id'][0]\n",
    "    if time_first == 0:\n",
    "        time_first = date_time\n",
    "    time_last = date_time\n",
    "    if '/ack/download' in l:\n",
    "        downloaded_sras.add(sra_id)\n",
    "    elif '/ack/build' in l:\n",
    "        build_sras.add(sra_id)\n",
    "    elif '/ack/clean' in l:\n",
    "        clean_sras.add(sra_id)\n",
    "    elif '/ack/transfer' in l:\n",
    "        transfer_sras.add(sra_id)\n",
    "    elif '/nack/download' in l:\n",
    "        ndownloaded_sras.add(sra_id)\n",
    "    elif '/nack/build' in l:\n",
    "        nbuild_sras.add(sra_id)\n",
    "    elif '/nack/clean' in l:\n",
    "        nclean_sras.add(sra_id)\n",
    "    elif '/nack/transfer' in l:\n",
    "        ntransfer_sras.add(sra_id)\n",
    "\n",
    "# remove SRAs that were re-tried and successfully processed after an initial failure\n",
    "ndownloaded_sras = ndownloaded_sras.difference(downloaded_sras)\n",
    "nbuild_sras = nbuild_sras.difference(build_sras)\n",
    "nclean_sras = nclean_sras.difference(clean_sras)\n",
    "print(f'Downloaded {len(downloaded_sras)}, Built {len(build_sras)}, Cleaned {len(clean_sras)}, Transferred {len(transfer_sras)}')\n",
    "print(f'Not downloaded {len(ndownloaded_sras)}, Not built {len(nbuild_sras)}, Not cleaned {len(nclean_sras)}, Not transferred {len(ntransfer_sras)}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:52:16.523137Z",
     "start_time": "2020-04-29T08:52:15.424324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of all SRAs, including not downloaded: 81.71TB\n",
      "Total size of downloaded SRAS: 58.36TB\n",
      "Total download size (of finished SRAs): 20.54MB\n",
      "Total not downloaded size: 24.49MB\n",
      "Total download time (of finished SRAs): 4803366s\n",
      "Download bandwidth (of finished SRAs): 4.48MB/s/machine\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import urllib.parse\n",
    "\n",
    "def format_bytes(size):\n",
    "    # 2**10 = 1024\n",
    "    power = 2**10\n",
    "    n = 0\n",
    "    power_labels = {0 : '', 1: 'K', 2: 'M', 3: 'G', 4: 'T', 5: 'P'}\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        n += 1\n",
    "    return str(round(size,2)) + power_labels[n]+'B'\n",
    "\n",
    "download_size_processed = 0\n",
    "ndownload_size = 0\n",
    "download_size_downloaded = 0\n",
    "size_all = 0\n",
    "download_size_processed_hist = defaultdict(int)\n",
    "download_size_processeds = []\n",
    "download_time = 0\n",
    "sra_to_size = {}\n",
    "ndownload_size_hist = defaultdict(int)\n",
    "coverage = []\n",
    "coverage_size = []\n",
    "coverage_total_size = 0\n",
    "seen_acks = set()\n",
    "seen_nacks = set()\n",
    "for l in lines:\n",
    "    if not 'ack/down' in l:\n",
    "        continue\n",
    "    parsed = urllib.parse.parse_qs(l.split(' ')[4])\n",
    "    sra_id = parsed['id'][0]\n",
    "    size = float(parsed['download_size_mb'][0])\n",
    "    \n",
    "    if '/ack/download' in l and sra_id in downloaded_sras:\n",
    "        if sra_id in seen_acks:\n",
    "            continue\n",
    "        \n",
    "        seen_acks.add(sra_id)\n",
    "        kmer_coverage = float(parsed['kmer_coverage'][0])\n",
    "        unique_kmers = float(parsed['kmer_count_unique'][0])\n",
    "        coverage.append(int(kmer_coverage))\n",
    "        coverage_size.append(size)\n",
    "        coverage_total_size += size\n",
    "        download_size_processed_hist[int(size/100) if int(size/100)<200 else 200] += 1\n",
    "        download_size_processeds.append(size/1e3)\n",
    "        sra_to_size[sra_id] = size\n",
    "        download_size_downloaded += size\n",
    "        \n",
    "        if sra_id in transfer_sras:\n",
    "            download_time += int(l.split('&')[2].split('=')[1])\n",
    "            download_size_processed += size\n",
    "    elif '/nack/download' in l and sra_id in ndownloaded_sras:\n",
    "        if sra_id in seen_nacks:  # download attempt that failed twice (e.g. after a re-process of failed downloads)\n",
    "            continue\n",
    "        seen_nacks.add(sra_id)\n",
    "        ndownload_size_hist[int(size/100) if int(size/100)<200 else 200] += 1\n",
    "        size_all += size\n",
    "        ndownload_size += size\n",
    "size_all += download_size_downloaded\n",
    "print(f'Total size of all SRAs, including not downloaded: {format_bytes(1e6 * size_all)}')\n",
    "print(f'Total size of downloaded SRAS: {format_bytes(1e6*download_size_downloaded)}')\n",
    "print(f'Total download size (of finished SRAs): {format_bytes(download_size_processed)}')\n",
    "print(f'Total not downloaded size: {format_bytes(ndownload_size)}')\n",
    "print(f'Total download time (of finished SRAs): {download_time}s')\n",
    "print(f'Download bandwidth (of finished SRAs): {round(download_size_processed/download_time,2)}MB/s/machine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:54:19.926993Z",
     "start_time": "2020-04-29T08:54:19.329931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total build time (of finished SRAs): 6425499s\n",
      "\n",
      "Total SRA size of built SRAs: 41.25TB\n",
      "Total SRA size of failed SRAs: 133.63GB\n",
      "Total SRA size of too large SRAs:  11.13TB \n",
      "\n",
      "Total build size (of finished SRAs): 2.22GB\n",
      "Build bandwidth is 3.352411977653426MB/s\n"
     ]
    }
   ],
   "source": [
    "time = 0\n",
    "size_build = 0\n",
    "size_build_sra = 0\n",
    "size_not_built_sra = 0\n",
    "build_size_hist = defaultdict(int)\n",
    "build_sizes = []\n",
    "build_size_to_time = {}\n",
    "nbuild_size_hist = defaultdict(int)\n",
    "nbuild_sizes = []\n",
    "too_large = 0\n",
    "nacked_builds = set()\n",
    "acked_builds = set()\n",
    "\n",
    "for d in lines:\n",
    "    if not 'ack/build' in d:\n",
    "        continue\n",
    "    sz = sra_to_size[sra_id]\n",
    "    parsed = urllib.parse.parse_qs(d.split(' ')[4])\n",
    "    sra_id = parsed['id'][0]\n",
    "    \n",
    "    if '/ack/build' in d:\n",
    "        if sra_id in acked_builds:  # account for double acks (e.g. when retrying a failed clean)\n",
    "            continue\n",
    "        acked_builds.add(sra_id)\n",
    "        \n",
    "        build_size_hist[int(sz/100) if int(sz/100)<200 else 200] += 1\n",
    "        build_sizes.append(sz/1e3)\n",
    "        tm = int(parsed['time'][0])\n",
    "        size_build = float(parsed['size_mb'][0])\n",
    "        size_build_sra += sz\n",
    "        build_size_to_time[sz] = tm\n",
    "        if sra_id in transfer_sras:\n",
    "            time = time + tm      \n",
    "    elif '/nack/build' in d:\n",
    "        if sra_id in nacked_builds or sra_id in build_sras:\n",
    "            continue\n",
    "        nacked_builds.add(sra_id)\n",
    "        nbuild_size_hist[int(sz/100) if int(sz/100)<200 else 200] += 1\n",
    "        nbuild_sizes.append(sz/1e3)\n",
    "        if 'required_ram_gb' in parsed:\n",
    "            too_large += sz\n",
    "        else:\n",
    "            size_not_built_sra += sz\n",
    "\n",
    "print(f'Total build time (of finished SRAs): {time}s\\n')\n",
    "print(f'Total SRA size of built SRAs: {format_bytes(1e6 * size_build_sra)}')\n",
    "print(f'Total SRA size of failed SRAs: {format_bytes(1e6 * size_not_built_sra)}')\n",
    "print(f'Total SRA size of too large SRAs: ', format_bytes(1e6 * too_large), '\\n')\n",
    "print(f'Total build size (of finished SRAs): {format_bytes(1e6 * size_build)}')\n",
    "print(f'Build bandwidth is {download_size_processed/time}MB/s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:19:14.716967Z",
     "start_time": "2020-04-29T11:19:14.304346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clean time (of finished SRAs): 15077410s\n",
      "Clean bandwidth is 21540919.81000011 15077410 1.43MB/s\n",
      "Total size of cleaned graphs:  723.15GB\n",
      "Compression factor is 27.741755287315513\n"
     ]
    }
   ],
   "source": [
    "time = 0\n",
    "clean_size = 0\n",
    "for d in lines:\n",
    "    if not 'ack/clean' in d:\n",
    "        continue\n",
    "    parsed = urllib.parse.parse_qs(d.split(' ')[4])\n",
    "    sra_id = parsed['id'][0]\n",
    "    \n",
    "    if sra_id in transfer_sras:\n",
    "        time = time + int(parsed['time'][0])\n",
    "        clean_size = clean_size + float(parsed['size_mb'][0])\n",
    "        \n",
    "    \n",
    "print(f'Total clean time (of finished SRAs): {time}s')\n",
    "print (f'Clean bandwidth is {download_size_processed} {time} {round(download_size_processed/time,2)}MB/s')\n",
    "print(f'Total size of cleaned graphs: ', format_bytes(clean_size*1e6))\n",
    "print(f'Compression factor is {download_size_processed/clean_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T07:16:16.354480Z",
     "start_time": "2020-04-29T07:16:16.351436Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.bar(list(download_size_processed_hist.keys()), download_size_processed_hist.values(), color='g')\n",
    "# plt.ylabel('count')\n",
    "# plt.xlabel('size (100s of MB)')\n",
    "# plt.title('SRA distribution by size (download successful)')\n",
    "# plt.show()\n",
    "\n",
    "# plt.bar(list(ndownload_size_hist.keys()), ndownload_size_hist.values(), color='g')\n",
    "# plt.ylabel('count')\n",
    "# plt.xlabel('size (100s of MB)')\n",
    "# plt.title('SRA distribution by size (download failed)')\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(download_size_processeds, bins=range(0,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T07:16:16.360334Z",
     "start_time": "2020-04-29T07:16:16.357444Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.bar(list(build_size_hist.keys()), build_size_hist.values(), color='g')\n",
    "# plt.ylabel('count')\n",
    "# plt.xlabel('size (100s of MB)')\n",
    "# plt.title('SRA distribution by size (build successful)')\n",
    "# #plt.show()\n",
    "\n",
    "# plt.figure(2)\n",
    "# plt.bar(list(nbuild_size_hist.keys()), nbuild_size_hist.values(), color='g')\n",
    "# plt.ylabel('count')\n",
    "# plt.xlabel('size (100s of MB)')\n",
    "# plt.title('SRA distribution by size (build failed)')\n",
    "# #plt.show()\n",
    "\n",
    "# plt.figure(3)\n",
    "# plt.title('SRA histogram')\n",
    "# plt.hist(build_sizes, bins=[0, 10, 20, 30, 40, 50])\n",
    "# plt.hist(nbuild_sizes, bins=[0, 10, 20, 30, 40, 50])\n",
    "# #plt.show()\n",
    "\n",
    "# plt.figure(4)\n",
    "# plt.title('Build time by size')\n",
    "# od = collections.OrderedDict(sorted(build_size_to_time.items()))\n",
    "# # plt.plot(list(od.keys()), list(od.values()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T07:16:16.366240Z",
     "start_time": "2020-04-29T07:16:16.363170Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure(1)\n",
    "# plt.title('K-mer coverage')\n",
    "# plt.hist(coverage, bins=range(0,40), weights=coverage_size, density=True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
